{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fast_Style_NST.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajraymond27/entertAIn/blob/master/Fast_Style_NST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cswKy-6zg-zk"
      },
      "source": [
        "# **Fast NeuralStyleTransfer Model**\n",
        "---\n",
        "This notebook is for D.ai TRIP MEDIA use only. If you are accessing this notebook and wish to try it out, please make a copy in your personal Drive to work from. Thanks! \n",
        "---\n",
        "---\n",
        "About this notebook:\n",
        "---\n",
        "---\n",
        "Neural style transfer is an optimization technique used to take two images—a content image and a style reference image (such as an artwork by a famous painter)—and blend them together so the output image looks like the content image, but “painted” in the style of the style reference image.\n",
        "\n",
        "This is implemented by optimizing the output image to match the content statistics of the content image and the style statistics of the style reference image. These statistics are extracted from the images using a convolutional network.\n",
        "\n",
        "For example, let’s take an image of this dog and Wassily Kandinsky's Composition 7:\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg\" width=\"500px\"/>\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg\" width=\"500px\"/>\n",
        "\n",
        "Now how would it look like if Kandinsky decided to paint the picture of this Dog exclusively with this style? Something like this?\n",
        "\n",
        "<img src=\"https://tensorflow.org/tutorials/generative/images/stylized-image.png\" style=\"width: 500px;\"/>\n",
        "\n",
        "\n",
        "---\n",
        "How to use this notebook:\n",
        "---\n",
        "---\n",
        "For most AI models, the trickiest part is shaping the data in the exact form required by the model itself. You can't fit a square peg through a round hole. Here is a step by step guide to style your video content: \n",
        "\n",
        "**Step 1:** Select a video and style image\n",
        "- Video will require preprocessing (see step 2)\n",
        "- Save style image in PNG format\n",
        "\n",
        "**Step 2:** Preprocess Video\n",
        "- Import video to Adobe Premiere Pro (or other video editing service)\n",
        "- Export video as a PNG image sequence \n",
        "\n",
        "**Step 3:** Save your input content in a folder structured like this: \n",
        "- example (you can name this folder anything you want. just keep it consistent)\n",
        "\n",
        "> - content\n",
        "\n",
        ">> - img_000\n",
        ">> - img_001\n",
        ">> - img_002\n",
        ">> - img_003\n",
        ">> - ... and all other images pulled from your video\n",
        "\n",
        "> - style\n",
        "\n",
        ">> - style.png\n",
        "\n",
        "**Step 4:** Zip / Compress 'example' folder and upload to your Google Drive\n",
        "- 'example.zip'\n",
        "- Must be in MyDrive (not shared folder)\n",
        "\n",
        "**Step 5:** Create a folder in the Fast NST --> Output folder with the same name you've been using \n",
        "- 'example'\n",
        "\n",
        "**Step 6:** In this notebook, enable GPU by clicking Runtime --> Change runtime type --> Hardware accelerator = GPU; Runtime shape = HighRAM --> Save\n",
        "\n",
        "- (NOTE: This model requires [colab pro](https://colab.research.google.com/signup) to process most video-length content. Regardless, it's suggested you process your image sequence in batches as described in step 6)\n",
        "\n",
        "**Step 7:** Look forward in this notebook to update the name of your .zip file and set your batch size as explained in the code blocks\n",
        "- (NOTE: With colab pro, you should be able to style several hundred images in each batch. Higher the resolution, lower the batch size. If your batch size is too large, you will see an error message stating you exceeded your RAM limit. Without colab pro, you wil only be able to process a few at a time) \n",
        "\n",
        "**Step 8:** Select Runtime --> Run all\n",
        "- Watch each code block below as your image is styled through the model\n",
        "- The last block will save your styled images to the Output --> example folder you created earlier\n",
        "\n",
        "**Step 9:** Continue styling your images in batches by updating your batch settings\n",
        "- After you changes your batch size settings, press play on the code block along with all the ones beneath\n",
        "- Continue this step until all images are styled\n",
        "\n",
        "**Step 10:** Download your Output --> example folder from Google Drive and complete your video editing process in your preferred video editing service\n",
        "\n",
        "---\n",
        "Start Creating! \n",
        "---\n",
        "---\n",
        "As you use this notebook, keep giving AJ feedback & ideas on how to make this process easier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCiGImELD4uF"
      },
      "source": [
        "# **Enabling and Testing GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPtvvu3lDvIA",
        "cellView": "form"
      },
      "source": [
        "#@title Hit This GPU Bruh\n",
        "#Check if GPU is connected\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# How much GPU, tho?\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# What about RAM?\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTA9YBBID-Jz"
      },
      "source": [
        "# **Import & Configure Libraries & Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU4--JBGD9f4",
        "cellView": "form"
      },
      "source": [
        "#@title Art Vandalay, Importer / Exporter\n",
        "import os\n",
        "from os.path import exists\n",
        "# Load compressed models from tensorflow_hub\n",
        "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
        "tf.__version__\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import IPython.display as display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import functools\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoc59U8jEIPr"
      },
      "source": [
        "# **Set Path to Content and Style Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smzt3v9yEJmD",
        "cellView": "form"
      },
      "source": [
        "#@title Download Google\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ox0Z_9ZELhd",
        "cellView": "form"
      },
      "source": [
        "#@title Authenticate Yourself\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OsDsP7gENFc",
        "cellView": "form"
      },
      "source": [
        "#@title Search for your .zip file in Google Drive folder\n",
        "#@markdown ---\n",
        "#@markdown Format the two fields like this (copy/paste):\n",
        "\n",
        "#@markdown query: title = 'example.zip'\n",
        "\n",
        "#@markdown zip: example.zip\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "query = \"title='example.zip'\" #@param {type:\"string\"}\n",
        "zip = \"example.zip\" #@param {type:\"string\"}\n",
        "\n",
        "fid = drive.ListFile({'q': query}).GetList()[0]['id']\n",
        "f = drive.CreateFile({'id': fid})\n",
        "f.GetContentFile(zip)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw26fStDEPhg",
        "cellView": "form"
      },
      "source": [
        "#@title Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNVgnhrNfwOu"
      },
      "source": [
        "# **Load Content and Style Images**\n",
        "\n",
        "## Unzip your .zip\n",
        "---\n",
        "Change \"example.zip\" to your zip file name. The code below should look like this:\n",
        "\n",
        "f.keys()\n",
        "\n",
        "!unzip example.zip\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-yUlX0dERF4",
        "cellView": "code"
      },
      "source": [
        "f.keys()\n",
        "!unzip example.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzb89Q-1oape",
        "cellView": "form"
      },
      "source": [
        "#@title Set Batch Size\n",
        "#@markdown ---\n",
        "first_image =  0#@param {type:\"integer\"}\n",
        "last_image =  100#@param {type:\"integer\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Cy8N3yCfFkZM"
      },
      "source": [
        "#@title Enter your desired output resolution\n",
        "#@markdown ---\n",
        "#@markdown Value should be width if in landscape orientation\n",
        "\n",
        "#@markdown Value should be hight if in portrait orientation\n",
        "\n",
        "#@markdown Example: 1920x1080px ... Value = 1920\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "max_dimension =  1920#@param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNfim1UxEVFy",
        "cellView": "form"
      },
      "source": [
        "#@title Download all content and style images\n",
        "\n",
        "PATH = '/content/' + zip.strip(\".zip\")\n",
        "\n",
        "if exists(os.path.join(PATH,\n",
        "                          'content/.DS_Store')): \n",
        "  os.remove(os.path.join(PATH,\n",
        "                          'content/.DS_Store'))\n",
        "\n",
        "content_path = os.path.join(PATH,\n",
        "                         'content/')\n",
        "\n",
        "style_path = os.path.join(PATH,\n",
        "                        'style/style.png')\n",
        "\n",
        "def tensor_to_image(tensor):\n",
        "  tensor = tensor*255\n",
        "  tensor = np.array(tensor, dtype=np.uint8)\n",
        "  if np.ndim(tensor)>3:\n",
        "    assert tensor.shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return PIL.Image.fromarray(tensor)\n",
        "\n",
        "def load_img(path_to_img):\n",
        "  max_dim = max_dimension\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  img = tf.image.resize(img, new_shape)\n",
        "  img = img[tf.newaxis, :]\n",
        "  return img\n",
        "\n",
        "def imshow(image, title=None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)\n",
        "\n",
        "content_images = []\n",
        "image_paths = sorted(os.listdir(content_path))\n",
        "\n",
        "# iterate through the names of contents of the folder\n",
        "for image_path in image_paths[first_image:last_image]:\n",
        "\n",
        "  # create the full input path and read the file\n",
        "  input_path = os.path.join(content_path, image_path)\n",
        "  content_image = load_img(input_path)\n",
        "\n",
        "  content_images.append(content_image)\n",
        "\n",
        "\n",
        "style_image = load_img(style_path)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_images[0], 'Content Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_image, 'Style Image')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVFwom_KEngP"
      },
      "source": [
        "# Fast Style Transfer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeBz1tp1En-t",
        "cellView": "form"
      },
      "source": [
        "#@title Run Fast Style Transfer\n",
        "import tensorflow_hub as hub\n",
        "hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
        "\n",
        "fast_styled_images = []\n",
        "start = time.time()\n",
        "\n",
        "for i in content_images:\n",
        "  stylized_image = hub_model(tf.constant(i), tf.constant(style_image))[0]\n",
        "  tensor_to_image(stylized_image)\n",
        "  fast_styled_images.append(stylized_image)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total time: {:.1f}\".format(end-start))\n",
        "print(\"Number of Styled Frames: {}\".format(len(fast_styled_images)))\n",
        "print(\"Example:\")\n",
        "# tensor_to_image(fast_styled_images[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o8dgWZXE1tV"
      },
      "source": [
        "# Save Styled Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUIRrtg4ZBaQ",
        "cellView": "form"
      },
      "source": [
        "#@title Save output to company drive folder\n",
        "import os\n",
        "os.chdir('/content/drive/Shareddrives/D.ai TRIP MEDIA/D.ai MODELS/Production Models/FAST_STYLE_NST/Output/' + zip.strip(\".zip\"))\n",
        "!pwd\n",
        "\n",
        "count = first_image\n",
        "\n",
        "for i in fast_styled_images:\n",
        "  file_name = 'fast-styled-image-' + str(count) + \".png\"\n",
        "  tensor_to_image(i).save(file_name)\n",
        "  files.download(file_name)\n",
        "  count += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}