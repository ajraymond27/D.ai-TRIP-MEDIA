{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep Dream.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajraymond27/D.ai-TRIP-MEDIA/blob/master/Deep_Dream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7gifg92NbG9",
        "cellView": "form"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "LpkCCNdS_Pjz"
      },
      "source": [
        "#@title Hit This GPU Bruh\n",
        "#Check if GPU is connected\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# How much GPU, tho?\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# What about RAM?\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc5Yq_Rgxreb",
        "cellView": "form"
      },
      "source": [
        "#@title Art Vandalay, Importer Exporter\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import IPython.display as display\n",
        "import PIL.Image\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import cv2\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5BPgc8NNbG0",
        "cellView": "form"
      },
      "source": [
        "#@title Download, Deprocess, and Show\n",
        "# Download an image and read it into a NumPy array.\n",
        "def download(path, max_dim=None):\n",
        "  img = PIL.Image.open(path)\n",
        "  if max_dim:\n",
        "    img.thumbnail((max_dim, max_dim))\n",
        "  return np.array(img)\n",
        "\n",
        "# Normalize an image\n",
        "def deprocess(img):\n",
        "  img = 255*(img + 1.0)/2.0\n",
        "  return tf.cast(img, tf.uint8)\n",
        "\n",
        "# Display an image\n",
        "def show(img):\n",
        "  display.display(PIL.Image.fromarray(np.array(img)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlLi48GKNbGy",
        "cellView": "form"
      },
      "source": [
        "#@title Prepare the feature extraction model\n",
        "#@markdown Download and prepare a pre-trained image classification model. You will use InceptionV3 which is similar to the model originally used in DeepDream. Note that any pre-trained model will work, although you will have to adjust the layer names below if you change this.\n",
        "\n",
        "#@markdown The idea in DeepDream is to choose a layer (or layers) and maximize the \"loss\" in a way that the image increasingly \"excites\" the layers. The complexity of the features incorporated depends on layers chosen by you, i.e, lower layers produce strokes or simple patterns, while deeper layers give sophisticated features in images, or even whole objects.\n",
        "\n",
        "#@markdown The InceptionV3 architecture is quite large (for a graph of the model architecture see TensorFlow's research repo). For DeepDream, the layers of interest are those where the convolutions are concatenated. There are 11 of these layers in InceptionV3, named 'mixed0' though 'mixed10'. Using different layers will result in different dream-like images. Deeper layers respond to higher-level features (such as eyes and faces), while earlier layers respond to simpler features (such as edges, shapes, and textures). Feel free to experiment with the layers selected below, but keep in mind that deeper layers (those with a higher index) will take longer to train on since the gradient computation is deeper.\n",
        "\n",
        "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "\n",
        "# Maximize the activations of these layers\n",
        "names = ['mixed3', 'mixed5']\n",
        "layers = [base_model.get_layer(name).output for name in names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "dream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MhfSweXXiuq",
        "cellView": "form"
      },
      "source": [
        "#@title Calculate Loss\n",
        "#@markdown The loss is the sum of the activations in the chosen layers. The loss is normalized at each layer so the contribution from larger layers does not outweigh smaller layers. Normally, loss is a quantity you wish to minimize via gradient descent. In DeepDream, you will maximize this loss via gradient ascent.\n",
        "\n",
        "def calc_loss(img, model):\n",
        "  # Pass forward the image through the model to retrieve the activations.\n",
        "  # Converts the image into a batch of size 1.\n",
        "  img_batch = tf.expand_dims(img, axis=0)\n",
        "  layer_activations = model(img_batch)\n",
        "  if len(layer_activations) == 1:\n",
        "    layer_activations = [layer_activations]\n",
        "\n",
        "  losses = []\n",
        "  for act in layer_activations:\n",
        "    loss = tf.math.reduce_mean(act)\n",
        "    losses.append(loss)\n",
        "\n",
        "  return  tf.reduce_sum(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRScWg_VNqvj",
        "cellView": "form"
      },
      "source": [
        "#@title Gradient Ascent\n",
        "#@markdown Once you have calculated the loss for the chosen layers, all that is left is to calculate the gradients with respect to the image, and add them to the original image.\n",
        "\n",
        "#@markdown Adding the gradients to the image enhances the patterns seen by the network. At each step, you will have created an image that increasingly excites the activations of certain layers in the network.\n",
        "\n",
        "#@markdown The method that does this, below, is wrapped in a tf.function for performance. It uses an input_signature to ensure that the function is not retraced for different image sizes or steps/step_size values. See the Concrete functions guide for details.\n",
        "\n",
        "\n",
        "class DeepDream(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(\n",
        "      input_signature=(\n",
        "        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=[], dtype=tf.int32),\n",
        "        tf.TensorSpec(shape=[], dtype=tf.float32),)\n",
        "  )\n",
        "  def __call__(self, img, steps, step_size):\n",
        "      print(\"Tracing\")\n",
        "      loss = tf.constant(0.0)\n",
        "      for n in tf.range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "          # This needs gradients relative to `img`\n",
        "          # `GradientTape` only watches `tf.Variable`s by default\n",
        "          tape.watch(img)\n",
        "          loss = calc_loss(img, self.model)\n",
        "\n",
        "        # Calculate the gradient of the loss with respect to the pixels of the input image.\n",
        "        gradients = tape.gradient(loss, img)\n",
        "\n",
        "        # Normalize the gradients.\n",
        "        gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
        "        \n",
        "        # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n",
        "        # You can update the image by directly adding the gradients (because they're the same shape!)\n",
        "        img = img + gradients*step_size\n",
        "        img = tf.clip_by_value(img, -1, 1)\n",
        "\n",
        "      return loss, img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vHEcy7dTysi",
        "cellView": "form"
      },
      "source": [
        "#@title Main Loop\n",
        "deepdream = DeepDream(dream_model)\n",
        "\n",
        "def run_deep_dream_simple(img, steps=100, step_size=0.01):\n",
        "  # Convert from uint8 to the range expected by the model.\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "  img = tf.convert_to_tensor(img)\n",
        "  step_size = tf.convert_to_tensor(step_size)\n",
        "  steps_remaining = steps\n",
        "  step = 0\n",
        "  while steps_remaining:\n",
        "    if steps_remaining>100:\n",
        "      run_steps = tf.constant(100)\n",
        "    else:\n",
        "      run_steps = tf.constant(steps_remaining)\n",
        "    steps_remaining -= run_steps\n",
        "    step += run_steps\n",
        "\n",
        "    loss, img = deepdream(img, run_steps, tf.constant(step_size))\n",
        "    \n",
        "    display.clear_output(wait=True)\n",
        "    show(deprocess(img))\n",
        "    print (\"Step {}, loss {}\".format(step, loss))\n",
        "\n",
        "\n",
        "  result = deprocess(img)\n",
        "  display.clear_output(wait=True)\n",
        "  show(result)\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "4LO3GVMt-ZSK"
      },
      "source": [
        "#@title Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eGDSdatLT-8",
        "cellView": "form"
      },
      "source": [
        "#@title Create Deep Dream Video From One Image\n",
        "#@markdown Using the File directory to the left, create a folder in Google Drive. Copy path to you folder and paste below:\n",
        "\n",
        "path = '/content/drive/Shareddrives/D.ai TRIP MEDIA/D.ai MODELS/In Development/Deep Dream/test' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Upload your starting image to your folder and rename it 'img_0000.jpg' (It must be .jpg for now)\n",
        "\n",
        "#@markdown Parameters:\n",
        "x_size = 600 #@param {type:\"number\"}\n",
        "y_size = 600 #@param {type:\"number\"}\n",
        "max_images =  300#@param {type:\"number\"}\n",
        "OCTAVE_SCALE = 1.10 #@param {type:\"number\"}\n",
        "x_trim =  2#@param {type:\"number\"}\n",
        "y_trim = 1 #@param {type:\"number\"}\n",
        "\n",
        "start = time.time()\n",
        "created_count = 0\n",
        "for i in range(0, 9999999999999999):\n",
        "\n",
        "    if os.path.isfile('{}/img_{:04}.jpg'.format(path, i+1)):\n",
        "        print('{} already exists, continuing along...'.format(i+1))\n",
        "\n",
        "    else:\n",
        "        dream_path = '{}/img_{:04}.jpg'.format(path, i)\n",
        "        img_result = download(dream_path, max_dim=max(x_size,y_size))\n",
        "\n",
        "        img_result = img_result[0+x_trim:y_size-y_trim, 0+y_trim:x_size-x_trim]\n",
        "        img_result = cv2.resize(img_result, (x_size, y_size))\n",
        "\n",
        "        # Use these to modify the general colors and brightness of results.\n",
        "        # results tend to get dimmer or brighter over time, so you want to\n",
        "        # manually adjust this over time.\n",
        "\n",
        "        # # +2 is slowly dimmer\n",
        "        # # +3 is slowly brighter\n",
        "        # img_result[:, :, 0] += 3  # reds\n",
        "        # img_result[:, :, 1] += 3  # greens\n",
        "        # img_result[:, :, 2] += 3  # blues\n",
        "\n",
        "        img_result = np.clip(img_result, 0.0, 255.0)\n",
        "        img_result = img_result.astype(np.uint8)\n",
        "\n",
        "        img_result = tf.constant(np.array(img_result))\n",
        "        base_shape = tf.shape(img_result)[:-1]\n",
        "        float_base_shape = tf.cast(base_shape, tf.float32)\n",
        "\n",
        "        for n in range(-2, 3):\n",
        "          new_shape = tf.cast(float_base_shape*(OCTAVE_SCALE**n), tf.int32)\n",
        "\n",
        "          img_result = tf.image.resize(img_result, new_shape).numpy()\n",
        "\n",
        "          img_result = run_deep_dream_simple(img=img_result, steps=5, step_size=0.01)\n",
        "\n",
        "        display.clear_output(wait=True)\n",
        "        img_result = tf.image.resize(img_result, base_shape)\n",
        "        img_result = tf.image.convert_image_dtype(img_result/255.0, dtype=tf.uint8)\n",
        "        img_result = np.clip(img_result, 0.0, 255.0)\n",
        "        img_result = img_result.astype(np.uint8)\n",
        "        result = PIL.Image.fromarray(img_result, mode='RGB')\n",
        "        result.save('{}/img_{:04}.jpg'.format(path, i+1))\n",
        "\n",
        "        created_count += 1\n",
        "        if created_count > max_images:\n",
        "            break\n",
        "\n",
        "end = time.time()\n",
        "end-start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cBT3arWp9jvR"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from PIL import ImageFile, Image\n",
        "\n",
        "#@markdown **Generate a video with the result (You can edit frame rate and stuff by double-clicking this tab)**\n",
        "first_frame = 0#@param {type:\"number\"} #This is the frame where the video will start\n",
        "last_frame = 300 #@param {type:\"number\"} #You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
        "\n",
        "# min_fps = 30#@param {type:\"number\"}\n",
        "# max_fps = 30#@param {type:\"number\"}\n",
        "\n",
        "# total_frames = last_frame-init_frame\n",
        "\n",
        "# length = 5#@param {type:\"number\"} #Desired video time in seconds\n",
        "\n",
        "frames = []\n",
        "tqdm.write('Generating video...')\n",
        "for i in range(first_frame,last_frame): \n",
        "    filename = f\"{path}/img_{i:04}.jpg\"\n",
        "    frames.append(Image.open(filename))\n",
        "\n",
        "#fps = last_frame/10\n",
        "fps = 30#@param {type:\"number\"}\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', 'video.mp4'], stdin=PIPE)\n",
        "for im in tqdm(frames):\n",
        "    im.save(p.stdin, 'png')\n",
        "p.stdin.close()\n",
        "\n",
        "print(\"The video is now being compressed, wait...\")\n",
        "p.wait()\n",
        "print(\"The video is ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KXbpQGxpBVYa"
      },
      "source": [
        "#@markdown #**Download the result video**\n",
        "from google.colab import files\n",
        "files.download(\"video.mp4\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}